{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408c920c-b453-4462-8102-58cbe8188b87",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "201fe1cf-d414-482f-a82e-1bbb83b1ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec68e0f-761c-4c12-b15a-c3641d2ae316",
   "metadata": {},
   "source": [
    "# load images and labels from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd93dd27-f511-40c0-a41c-7bc5c20bb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images and their corresponding labels from the directory\n",
    "def load_data(directory):\n",
    "    images = []  # To store image data\n",
    "    labels = []  # To store corresponding labels\n",
    "    class_names = sorted(os.listdir(directory))  # Get class names in sorted order\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, file_name)\n",
    "                img = load_img(img_path, target_size=img_size)  # Load image and resize to specified size\n",
    "                img = img_to_array(img)  # Convert image to array format\n",
    "                images.append(img)  # Append image to the list\n",
    "                labels.append(label)  # Append label (numeric class index) to the list\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels, class_names\n",
    "\n",
    "# Define directories for training and testing datasets\n",
    "train_dir = 'C:\\\\Users\\\\dovy4\\\\Desktop\\\\jupyter test\\\\1000'\n",
    "test_dir = 'C:\\\\Users\\\\dovy4\\\\Desktop\\\\jupyter test\\\\test_100'\n",
    "\n",
    "# Define image size (32x32 pixels)\n",
    "img_size = (32, 32)\n",
    "\n",
    "# Load and shuffle training data\n",
    "train_images, train_labels, class_names = load_data(train_dir)\n",
    "train_images, train_labels = shuffle(train_images, train_labels, random_state=42)\n",
    "\n",
    "# Load and shuffle test data\n",
    "test_images, test_labels, _ = load_data(test_dir)\n",
    "test_images, test_labels = shuffle(test_images, test_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77046a3-ce5d-4d34-8c34-36cd121293ea",
   "metadata": {},
   "source": [
    "# Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41fe284a-e585-46dc-89db-8f1e303fb961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image data to range [0, 1] by dividing by 255\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f09c9e-a515-435d-a510-74ff73921bab",
   "metadata": {},
   "source": [
    "# One-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3384eba0-e17b-4a7a-b471-9ca4cd11b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding format\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=len(class_names))\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791df07-5cb9-4414-ac43-6532e6ce4f08",
   "metadata": {},
   "source": [
    "# Define the CNN model for 32x32 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cdb5f-a516-4d20-ba05-c78ffe9009fb",
   "metadata": {},
   "source": [
    "for this part more layers were initially added however this caused overfitting in the training process and a more simplified version actually gave us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "092925df-e870-43c7-95b9-a7faadbab894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model architecture\n",
    "model = models.Sequential([\n",
    "    # First convolutional layer with 32 filters, each of size 3x3\n",
    "    # Activation function: ReLU (Rectified Linear Unit)\n",
    "    # Input shape specified for the first layer: 32x32 image with 3 color channels (RGB)\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), padding='same'),\n",
    "    \n",
    "    # MaxPooling layer to reduce the spatial dimensions (downsampling)\n",
    "    # Pool size of 2x2, reduces the size of the feature maps by half\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Second convolutional layer with 64 filters, each of size 3x3\n",
    "    # Activation function: ReLU\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    \n",
    "    # MaxPooling layer to further reduce the spatial dimensions\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Third convolutional layer with 128 filters, each of size 3x3\n",
    "    # Activation function: ReLU\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    \n",
    "    # MaxPooling layer to reduce spatial dimensions for the last time\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten layer to convert 2D matrices into a 1D vector (required before feeding into fully connected layers)\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # Fully connected (Dense) layer with 256 units\n",
    "    # Activation function: ReLU\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    \n",
    "    # Dropout layer to prevent overfitting by randomly setting a fraction of input units to 0 during training\n",
    "    # Dropout rate: 50% (0.5)\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    # Output layer with units equal to the number of classes (class_names)\n",
    "    # Activation function: Softmax (for multi-class classification)\n",
    "    layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer, categorical crossentropy loss function, and accuracy metric\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549014b-e3f9-4807-a62c-d69b6a7d7a5a",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d1830c1-7248-433e-b2e8-00b6057cca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1951 - loss: 2.1345 - val_accuracy: 0.3715 - val_loss: 1.7231 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.3878 - loss: 1.6471 - val_accuracy: 0.4615 - val_loss: 1.4778 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.4692 - loss: 1.4624 - val_accuracy: 0.5085 - val_loss: 1.3917 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5164 - loss: 1.3305 - val_accuracy: 0.5315 - val_loss: 1.3081 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5518 - loss: 1.2270 - val_accuracy: 0.5595 - val_loss: 1.2465 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5865 - loss: 1.1334 - val_accuracy: 0.5910 - val_loss: 1.1788 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.6352 - loss: 1.0133 - val_accuracy: 0.5835 - val_loss: 1.1491 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6670 - loss: 0.9306 - val_accuracy: 0.5955 - val_loss: 1.1564 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6896 - loss: 0.8486 - val_accuracy: 0.6175 - val_loss: 1.1038 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7287 - loss: 0.7446 - val_accuracy: 0.6320 - val_loss: 1.1935 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7699 - loss: 0.6353 - val_accuracy: 0.6285 - val_loss: 1.1343 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8311 - loss: 0.4897 - val_accuracy: 0.6465 - val_loss: 1.1026 - learning_rate: 2.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8540 - loss: 0.4190 - val_accuracy: 0.6390 - val_loss: 1.1318 - learning_rate: 2.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8664 - loss: 0.3950 - val_accuracy: 0.6500 - val_loss: 1.1460 - learning_rate: 2.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8822 - loss: 0.3525 - val_accuracy: 0.6480 - val_loss: 1.1520 - learning_rate: 4.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8809 - loss: 0.3456 - val_accuracy: 0.6485 - val_loss: 1.1585 - learning_rate: 4.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8856 - loss: 0.3391 - val_accuracy: 0.6500 - val_loss: 1.1632 - learning_rate: 1.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8792 - loss: 0.3407 - val_accuracy: 0.6475 - val_loss: 1.1673 - learning_rate: 1.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8947 - loss: 0.3183 - val_accuracy: 0.6475 - val_loss: 1.1699 - learning_rate: 1.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8910 - loss: 0.3285 - val_accuracy: 0.6480 - val_loss: 1.1683 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Define a callback to reduce learning rate when the validation loss has stopped improving\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-5)\n",
    "\n",
    "# Train the model using the training data, with 20 epochs and a validation split of 20%\n",
    "# The reduce_lr callback is used to adjust the learning rate during training\n",
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=[reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331294c4-741d-4e89-8f4b-7b9d63bedaca",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "864d2dac-bbaf-463d-a597-803e349eda8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6852 - loss: 1.0735\n",
      "Test accuracy: 0.6729999780654907\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b47899-391f-4725-9a83-0641384b046b",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f5cf2-6b41-41f5-a479-c7ddc4650cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
